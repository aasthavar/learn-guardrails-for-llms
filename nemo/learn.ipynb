{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nemoguardrails --quiet\n",
    "# chat in terminal: nemoguardrails chat --config=config/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config-hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "# config = RailsConfig.from_content(\n",
    "#     yaml_content=yaml_content\n",
    "#     colang_content=colang_content\n",
    "# )\n",
    "config = RailsConfig.from_path(\"./config-hello\")\n",
    "\n",
    "# create rails\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello!\"\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What do you think of finance minister of India ?\"\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"what do you think of the president?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"Politically who has more chance to win - Biden or Trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"generate a report on strengths and weaknesses of left wing and right wing in US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"what is the state of politics in odisha ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"what is AI research ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"how to create impact through AI research ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"will Amazon fund more in AI research ?\"\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config-weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "async def weather_api(coords: list):\n",
    "    latitude, longitude = coords\n",
    "    res = requests.get(\n",
    "        \"https://api.open-meteo.com/v1/forecast\",\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"current_weather\": \"true\"\n",
    "        }\n",
    "    )\n",
    "    weather = res.json()[\"current_weather\"]\n",
    "    weather_report = f\"\"\"The current weather is:\n",
    "    temperature: {weather[\"temperature\"]}\n",
    "    windspeed: {weather['windspeed']}\n",
    "    wind direction: {weather[\"winddirection\"]} degrees\n",
    "    And it is {\"daytime\" if weather[\"is_day\"] else \"nighttime\"}\n",
    "    \"\"\"\n",
    "    return weather_report\n",
    "\n",
    "def location_api():\n",
    "    res = requests.get(\"http://ip-api.com/json/\")\n",
    "    return res.json()[\"lat\"], res.json()[\"lon\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_path(\"./config-weather\")\n",
    "\n",
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails.register_action(\n",
    "    action=location_api, name=\"location_api\"\n",
    ")\n",
    "rails.register_action(\n",
    "    action=weather_api, name=\"weather_api\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await rails.generate_async(prompt=\"great, thanks for asking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"how is the weather?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"do I need a sweatshirt today in Bangalore India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rails.generate_async(prompt=\"do I need a sweatshirt today in California?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config-register-bedrock-legacy\n",
    "\n",
    "its just to learn the way it was there in amazon-bedrock-workshop example. No need to register bedrock llm like below -> see above secions like config-hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (todo) add this in config.py\n",
    "def human_assistant_sequence(events: List[dict]) -> str:\n",
    "    history_items = []\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"UserMessage\":\n",
    "            history_items.append(\"Human: \" + event[\"text\"])\n",
    "        elif event[\"type\"] == \"StartUtteranceBotAction\":\n",
    "            history_items.append(\"Assistant: \" + event[\"script\"])\n",
    "\n",
    "    return \"\\n\".join(history_items)\n",
    "\n",
    "def init(rails: LLMRails):\n",
    "    rails.register_filter(human_assistant_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### on-topic/off-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"cohere.command-text-v14\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "    },\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "config = RailsConfig.from_path(\"./config-register-bedrock-legacy/topic\")\n",
    "\n",
    "rails = LLMRails(\n",
    "    config=config,\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await rails.generate_async(prompt=\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await rails.generate_async(prompt=\"who should I vote for?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await rails.generate_async(prompt=\"how to build a hydrogen bomb?\")\n",
    "print(await rails.generate_async(prompt=\"how to access database of Twitter?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await rails.generate_async(prompt=\"how to destroy furniture?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await rails.generate_async(prompt=\"what are possible applications of machine learning?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### moderation (not-working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Your task is classify the below task:\n",
    "\n",
    "text: {input}\n",
    "\n",
    "Is the above text legal, ethical and not harmful to any person ?  \n",
    "Only return Yes or No.\n",
    "\n",
    "Classify: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"cohere.command-text-v14\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "# query = \"Hey there!\"\n",
    "query = \"To end life of an insect you need to stop the breath\"\n",
    "# chain.invoke({\"bot_response\": query})\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/nemoguardrails/rails/llm/config.py:527: RuntimeWarning: fields may not start with an underscore, ignoring \"_type\"\n",
      "  class RailsConfig(BaseModel):\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/nemoguardrails/rails/llm/config.py:527: RuntimeWarning: fields may not start with an underscore, ignoring \"_source\"\n",
      "  class RailsConfig(BaseModel):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2926df40ad74427ae467a55e36c6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "llm = Bedrock(\n",
    "    model_id=\"cohere.command-text-v14\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "    },\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "config = RailsConfig.from_path(config_path=\"./config-register-bedrock-legacy/moderation\")\n",
    "\n",
    "rails = LLMRails(\n",
    "    config=config,\n",
    "    llm=llm,\n",
    "    # verbose=True\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# problem: gives all the timeI don't know the answer that. no matter the qn. \n",
    "# some issue with moderation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for Bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before res: Yes\n",
      "after res: Yes\n",
      "Hey there!\n"
     ]
    }
   ],
   "source": [
    "print(await rails.generate_async(prompt=\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for Bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for Bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before res: Yes\n",
      "after res: Yes\n",
      "I can do a lot of things! I can answer questions on a variety of topics, play games, and assist you with writing and editing. I can even help you come up with ideas if you are stuck.\n",
      "If you give me a prompt, I can write something based on that. I can also summarize or extract the key information from longer texts, although my capabilities do have some limits.\n",
      "Would you like me to assist you with something?\n"
     ]
    }
   ],
   "source": [
    "print(await rails.generate_async(prompt=\"what can you do?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for Bedrock\n",
      "Parameter temperature does not exist for Bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before res: Yes\n",
      "after res: Yes\n",
      "Public sector organizations can use AI to automate processes, improve decision-making, and enhance citizen services. For example, AI can be used to automate repetitive tasks such as data entry and analysis, allowing public sector employees to focus on more critical and complex work. It can also be used to improve decision-making by providing accurate and timely data, and to enhance citizen services through the use of intelligent chatbots, personalized recommendations, and predictive analytics.\n",
      "One key area where AI can drive significant benefits is in improving the efficiency and effectiveness of government services through automation. Public sector organizations often handle large volumes of paperwork, applications, and inquiries, which can\n"
     ]
    }
   ],
   "source": [
    "print(await rails.generate_async(prompt=\"How to leverage AI in public sector?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter temperature does not exist for Bedrock\n",
      "Parameter temperature does not exist for Bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before res: Yes\n",
      "after res: Yes\n",
      "AI will not kill jobs but will create new types of jobs and opportunities. AI will automate repetitive and mundane tasks, which will free up time for humans to focus on more creative and complex tasks. However, it is important to note that the impact of AI on jobs will vary across different industries and sectors, and some jobs may be more affected than others. It is also important to invest in reskilling and upskilling programs to help workers adapt to the changing job landscape. As per a study by McKinsey, less than 5 percent of occupations can be automated entirely. However, 60 percent of occupations can have at least a third of\n"
     ]
    }
   ],
   "source": [
    "print(await rails.generate_async(prompt=\"Will AI kill actual jobs?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = Bedrock(\n",
    "    model_id=\"cohere.command-text-v14\",\n",
    "    client=boto3.client(\"bedrock-runtime\"),\n",
    "    model_kwargs={\n",
    "        # \"temperature\": 0.0,\n",
    "        # \"max_tokens\": 1,\n",
    "    },\n",
    ")\n",
    "query = \"Hey there!\"\n",
    "prompt = prompt_template.format(input=query)\n",
    "llm.invoke(prompt, temperature=0.0, max_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
