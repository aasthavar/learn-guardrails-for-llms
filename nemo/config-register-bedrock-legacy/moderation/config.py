import boto3, sys, os, json
from typing import Optional
from langchain.llms.base import BaseLLM
from langchain.llms.bedrock import Bedrock
from nemoguardrails import LLMRails
from nemoguardrails.actions import action
from nemoguardrails.llm.providers import register_llm_provider
from nemoguardrails.llm.helpers import get_llm_instance_wrapper


bedrock_llm = Bedrock(
    model_id="cohere.command-text-v14",
    client=boto3.client("bedrock-runtime"),
    model_kwargs={}
)
# output_moderation_prompt_template = "model_output: {bot_response} \n\n Human: Is this output legal, ethical, unbiased to any person? Answer with yes/no \n\nAssistant:"   

output_moderation_prompt_template = """Your task is classify the below task:

text: {input}

Is the above text legal, ethical and not harmful to any person ?  
Only return Yes or No.

Classify: """

# @action()
async def bedrock_output_moderation(context: Optional[dict] = None,
        llm: Optional[BaseLLM] = None) -> bool:
    """
    Checks if the output generated by a language model is safe and adheres to moderation policies.

    Args:
        context: An optional dictionary containing the context of the conversation.
        llm: An optional instance of a language model.

    Returns:
        A boolean indicating if the output is safe and follows the moderation policy.
    """

    # TODO: Check if uer_input is malicious or not as well (anthropi.yml has some helper prompt)
    # user_input = context.get("last_user_message")
    # print(f"user_input: {user_input}")

    # print("context:")
    # print(json.dumps(context, indent=2, default=True))
    # return True
    
    bot_response = context.get("last_bot_message")
    
    output_moderation_prompt = output_moderation_prompt_template.format(
        input = bot_response
    )
    res = bedrock_llm.invoke(output_moderation_prompt, temperature=0.0, max_tokens=1)
    
    print(f"before res: {res}")
    res = res.strip().strip('.!')
    print(f"after res: {res}")
    
    return res.lower() in ['true', '1', 't', 'y', 'yes', 'yeah', 'yup', 'certainly', 'uh-huh', 'affirm']

    # Do not explain and you must only answer with "Yes" or "No". 



def init(app: LLMRails):

    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    app.register_action(bedrock_output_moderation, name="bedrock_output_moderation")

    llm_wrapper = get_llm_instance_wrapper(
        llm_instance=bedrock_llm, llm_type="bedrock_llm"
    )
    
    register_llm_provider("amazon_bedrock", llm_wrapper)
    